# 1. 初始数据集

import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_s_curve
import torch

s_curve, _=make_s_curve(10**4, noise=0.1)
s_curve=s_curve[:, [0, 2]]/10

print("shape", np.shape(s_curve))

data = s_curve.T

fig, ax=plt.subplots()
ax.scatter(*data, color='blue',edgecolor="white")
ax.axis("off")

dataset=torch.Tensor(s_curve).float()

# **2. 定义超参数**"""

num_steps=100

betas_step=torch.linspace(-6, 6, num_steps)
betas=torch.sigmoid(betas_step)*(0.5e-2-1e-5)+1e-5

alphas=1-betas
#累乘所得
alpha_prod=torch.cumprod(alphas,0)
alpha_prod_p=torch.cat([torch.tensor([1]).float(), alpha_prod[:-1]], 0)
alpha_bar_sqrt=torch.sqrt(alpha_prod)
# one_minus_bar_alpha_log=torch.log(1-alpha_prod)
one_minus_bar_alpha_sqrt=torch.sqrt(1-alpha_prod)

# **3. 确定任意时刻采样值**

def q_x(x_0, t):
  noise = torch.randn_like(x_0)
  alpha_sqrt_t = alpha_bar_sqrt[t]
  alpha_1_m_t = one_minus_bar_alpha_sqrt[t]
  return (alpha_sqrt_t*x_0 + alpha_1_m_t*noise)

# 4. 展示加噪结果**

num_shows=20
fig, axs=plt.subplots(2, 10, figsize=(28, 3))
plt.rc("text", color="blue")


for i in range(num_shows):
  j = i//10
  k = i%10
  q_i=q_x(dataset, torch.tensor([i*num_steps//num_shows]))
  data_t = q_i.T
  axs[j,k].scatter(q_i[:,0], q_i[:,1],color="red",edgecolor='white')
  axs[j,k].set_axis_off()
  axs[j,k].set_title('$q(\mathbf{x}_{'+str(i*num_steps//num_shows)+'})$')


# **5. MLP as backbone**

import torch
import torch.nn as nn

# define the structure of MLP
class Diffusion(nn.Module):

  def __init__(self, n_steps, num_units=128):
    super(Diffusion, self).__init__()
    # MLP
    self.linears=nn.ModuleList(
        [
        nn.Linear(2, num_units),
        nn.ReLU(),
        nn.Linear(num_units, num_units),
        nn.ReLU(),
        nn.Linear(num_units, num_units),
        nn.ReLU(),
        nn.Linear(num_units, 2),
        ]
    )
    self.step_embeddings=nn.ModuleList(
        [
            nn.Embedding(n_steps, num_units),
            nn.Embedding(n_steps, num_units),
            nn.Embedding(n_steps, num_units),
        ]
    )

  def forward(self, x, t):

    for idx, embedding_layer in enumerate(self.step_embeddings):
      t_embedding=embedding_layer(t)
      x=self.linears[2*idx](x)
      x+=t_embedding
      x=self.linears[2*idx+1](x)

    x=self.linears[-1](x)

    return x

# 6. Loss Function

def diffusion_loss_fn(model, x_0, alpha_bar_sqrt, one_minus_bar_alpha_sqrt, n_steps):
  batch_size=x_0.shape[0]

  # 随机采样时刻t,使其覆盖到更多的t
  t = torch.randint(0, n_steps, size=(batch_size//2,))
  t = torch.cat([t, n_steps-1-t], dim=0)
  t = t.unsqueeze(-1)

  a = alpha_bar_sqrt[t]
  am1 = one_minus_bar_alpha_sqrt[t]

  # 随机噪声
  e = torch.randn_like(x_0)

  x = x_0 * a + e * am1
  output=model(x, t.squeeze(-1))

  return (e-output).square().mean()

# 7. Inference

def p_sample_loop(model, shape, n_steps, betas, one_minus_bar_alpha_sqrt):
  cur_x=torch.randn(shape)
  x_seq=[cur_x]
  for i in reversed(range(n_steps)):
    cur_x=p_sample(model, cur_x, i, betas, one_minus_bar_alpha_sqrt)
    x_seq.append(cur_x)
  return x_seq

def p_sample(model, x, t, betas, one_minus_bar_alpha_sqrt):
  t = torch.tensor([t])
  coeff = betas[t]/one_minus_bar_alpha_sqrt[t]
  eps_theta = model(x, t)

  mean = (1/(1-betas[t]).sqrt())*(x-(coeff*eps_theta))

  z = torch.randn_like(x)
  sigma_t=betas[t].sqrt()
  sample=mean+sigma_t*z

  return (sample)

"""**8. 训练模型**"""

seed = 123

print("Training model...")
batch_size = 128
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
num_epoch = 4000
plt.rc("text", color="blue")

model = Diffusion(num_steps)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for t in range(num_epoch):
  # 单个epoch中训练
  for idx, batch_x in enumerate(dataloader):
    loss = diffusion_loss_fn(model, batch_x, alpha_bar_sqrt, one_minus_bar_alpha_sqrt, num_steps)
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)
    optimizer.step()

  # 展示inference结果
  if t%200==0:
    print(loss)
    x_seq = p_sample_loop(model, dataset.shape, num_steps, betas, one_minus_bar_alpha_sqrt)
    fig, axs = plt.subplots(1, 10, figsize=(28, 3))
    for i in range(1, 11):
      cur_x = x_seq[i * 10].detach()
      axs[i-1].scatter(cur_x[:, 0], cur_x[:, 1], color="red", edgecolor="white")
      axs[i-1].set_axis_off()
      axs[i-1].set_title('$q(\mathbf{x}_{'+str(i*10)+'})$')
